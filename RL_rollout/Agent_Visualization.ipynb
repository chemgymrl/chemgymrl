{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c5bae2a",
   "metadata": {},
   "source": [
    "# Default training Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bb9a1f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy                        \t\t\tMlpPolicy\n",
      "algorithm                     \t\t\tPPO\n",
      "environment                   \t\t\tWurtzReact-v1\n",
      "steps                         \t\t\t51200\n",
      "dir                           \t\t\t<DEFAULT>\n",
      "n_steps                       \t\t\t256\n",
      "n_envs                        \t\t\t1\n",
      "seed                          \t\t\tNone\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from RLTrain import Opt\n",
    "import os\n",
    "import time\n",
    "print(Opt())\n",
    "import pandas as pd\n",
    "from RadarGraph import *\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb426d9c",
   "metadata": {},
   "source": [
    "# Training an agent with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de2a407",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=time.time()\n",
    "#os.system(\"python RLTrain.py steps=200000\")\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9349c8c2",
   "metadata": {},
   "source": [
    "# Running inference with the trained models and saving [S,A,R,S] info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f26ab82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.system(\"python RLTest.py PPO_WurtzReact-v1 steps=500\")\n",
    "#os.system(\"python RLTest.py WRH algorithm=WRH steps=500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dc8f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "calc_return = default_obj = lambda x: x.Reward.sum()/x.Done.sum()\n",
    "worst_obj = lambda x: -x.Reward.sum()/x.Done.sum()\n",
    "\n",
    "\n",
    "def load_rollouts(env: str,obj = default_obj,last: bool = True,verbose:bool=False,TOL:float = 1e-4):\n",
    "    \"\"\"Retrieve RL rollouts of each algorithm from the file system\n",
    "    \n",
    "    Parameters:\n",
    "        obj (function) - Method to measure how good the rollout is\n",
    "        env (str) - The environment you want rollouts from\n",
    "        last (bool) - Use the rollout of the last timestep if true and the best performing timestep if false\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    folder = f\"MODELS\\\\{env}\" if len(env.split(\"\\\\\"))==1 else env\n",
    "    algoidx = [i for i,string in enumerate(folder.split(\"\\\\\")) if \"-v\" in string][0]\n",
    "    data = dict()\n",
    "    rollName=[\"best_rollout\",\"rollout\"][last]\n",
    "    for a,b,c in os.walk(folder):\n",
    "        if rollName in c:\n",
    "            algo = a.split(\"\\\\\")[algoidx+1]\n",
    "            df1 = pd.read_pickle(a+\"\\\\\"+rollName)\n",
    "            if verbose:print(a,\"|\",0 if obj is None else obj(df1))\n",
    "            if not algo in data:\n",
    "                data[algo]=df1\n",
    "            else:\n",
    "                df0=data[algo]\n",
    "                if obj is None:\n",
    "                    data[algo]=pd.concat([df0,df1],ignore_index=True)  \n",
    "                #prefer rollouts with more episodes when objective ranking is similar\n",
    "                elif abs(obj(df1)-obj(df0))<TOL:\n",
    "                    if verbose:print(df1.Done.sum(),df0.Done.sum(),df0.Done.shape)\n",
    "                    if df1.Done.sum()>df0.Done.sum():\n",
    "                        data[algo]=df1\n",
    "                #prefer higher ranking ones\n",
    "                elif obj(df1)>obj(df0):\n",
    "                    data[algo]=df1\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa7669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_subset(frame,N,i):\n",
    "    \"\"\"\n",
    "    Filters the rollout for episodes which have a specific target\n",
    "    Inputs:\n",
    "        Frame (dataframe) - Pandas Dataframe containing gym information\n",
    "        N (int)           - Number of targets in your environment\n",
    "        i (int)           - The index of your target as it appears in the observation space\n",
    "        \n",
    "    Outputs:\n",
    "        cframe (dataframe) - Subset of your Pandas dataframe with only episodes of target i\n",
    "    \n",
    "    \"\"\"\n",
    "    obs = np.stack(frame.InState)\n",
    "    cframe=frame[obs[:,-N+i]>0.9]\n",
    "    return cframe\n",
    "\n",
    "\n",
    "\n",
    "def actions_by_time(frame):\n",
    "    \"\"\"Gives the mean action at each timestep of your rollout dataframe\"\"\"\n",
    "    min_t,max_t = frame.Step.min(),frame.Step.max()\n",
    "    mean_act=[]\n",
    "    for t in range(min_t,max_t+1):\n",
    "        mean_act+=[frame.Action[frame.Step==t].mean()]\n",
    "    return np.array(mean_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad83f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotabt(all_act,actions,colors,names,title_ext=\"\",title=None,ylim=(-0.15,1.15)):\n",
    "    \"\"\"Plotting Function for actions vs time\n",
    "    Inputs:\n",
    "        all_act (list<np.array>) -- shape is [number of models, number of timesteps, number of actions]\n",
    "        actions (list<str>) -- list of the name of each action\n",
    "        colors (list<str>) -- list of colors (should be at least as long as the number of models)\n",
    "        names (list<str>) -- the names of each model\n",
    "        title_ext (str) -- addition you want to add to the end of the graph title\n",
    "    \n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(figsize=(9, 5), nrows=len(actions), ncols=len(all_act))\n",
    "    fig.subplots_adjust(wspace=0.0, hspace=0.0, top=0.85, bottom=0.05)\n",
    "    #i indexes the each action \n",
    "    for i,ax0 in enumerate(axs):\n",
    "        #j indexes each model\n",
    "        for j,act in enumerate(all_act):\n",
    "            #Model name\n",
    "            axs[0][j].set_title(names[j])\n",
    "            ax=ax0[j]\n",
    "            ax.plot(act[:,i],\".-\",color=colors[j],ms=3,alpha=0.8)\n",
    "            ax.set_ylim(*ylim)\n",
    "            if j!=0:\n",
    "                ax.set_yticks([])\n",
    "        #Action name name\n",
    "        ax.text(act.shape[0],ylim[1]*0.5,actions[i],horizontalalignment=\"right\",verticalalignment=\"top\"\n",
    "                ,bbox=dict(boxstyle=\"square\",facecolor=\"w\",edgecolor=\"k\",alpha=0.8))\n",
    "    if title is None:\n",
    "        axs[0][(int(len(names)-0.5)//2)-1].text(2,2,\"Average Value of Each Action vs Step %s\"%title_ext)\n",
    "    else:\n",
    "        axs[0][(int(len(names)-0.5)//2)-1].text(2,2,title)\n",
    "    #axs[0,-1].legend(names,loc=(0.8, .0))\n",
    "    axs[-1,0].set_xlabel(\"Step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abe69d2",
   "metadata": {},
   "source": [
    "# Gathering some Heuristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af4131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ppo = pd.read_pickle(\"Legacy/Legacy5/MODELS/WurtzReact-V1/PPO/06-03-2023--18-21-33/best_rollout\")\n",
    "heuristic = pd.read_pickle(\"Legacy/Legacy5/MODELS\\\\WurtzReact-v1\\\\Heuristic/rollout\")\n",
    "\n",
    "print(ppo.keys())\n",
    "\n",
    "info = [\n",
    "    ['dT', 'dV', '1-chlorohexane', '2-chlorohexane', '3-chlorohexane', 'Na' ],\n",
    "    ('PPO', [\n",
    "        [a for a in ppo.Action.mean()],\n",
    "        [ppo[ppo.Done==True].Reward.mean()]*6,\n",
    "        [1,0,0,0,0,0]]),\n",
    "    ('Heuristic', [\n",
    "        [a for a in heuristic.Action.mean()],\n",
    "        [heuristic[heuristic.Done==True].Reward.mean()]*6,\n",
    "        [1,0,0,0,0,0]]),\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e86e941",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(heuristic[heuristic.Done==True].Reward.mean())\n",
    "\n",
    "print(ppo[ppo.Done==True].Reward.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7675cb7c",
   "metadata": {},
   "source": [
    "# Plot as a Radar Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63302b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RadarGraph import *\n",
    "\n",
    "\n",
    "theta = radar_factory(len(info[0]), frame='polygon')\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(9, 9), nrows=1, ncols=2,subplot_kw=dict(projection='radar'))\n",
    "fig.subplots_adjust(wspace=0.5, hspace=0.25, top=0.85, bottom=0.05)\n",
    "\n",
    "make_radar(theta,axs,info,colors = \"brk\")\n",
    "\n",
    "labels = ('Action Taken', 'Return', '1', 'Factor 4', 'Factor 5')\n",
    "legend = axs[0].legend(labels, loc=(0.9, .95),labelspacing=0.1, fontsize='small')\n",
    "\n",
    "\n",
    "fig.text(0.5, 0.7, \"Average value of each action and episodic return for a 500 step rollout (PPO trained for 200K steps)\",\n",
    "             horizontalalignment='center', color='black', weight='bold',\n",
    "             size='large')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115f4b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_act = [actions_by_time(ppo),actions_by_time(heuristic)]\n",
    "actions = ['dT', 'dV', '1-chlorohexane', '2-chlorohexane', '3-chlorohexane', 'Na' ]\n",
    "colors = [\"r\",\"g\",\"b\",\"c\",\"y\",\"m\"]\n",
    "names=[\"PPO\",\"Heuristic\"]\n",
    "\n",
    "plotabt(all_act,actions,colors,names,title_ext=\"(WurtzReact PPO on 0.5M steps)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24656963",
   "metadata": {},
   "source": [
    "# Functions for Conditional Returns and Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c2ccc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chemistrylab.reactions.available_reactions.chloro_wurtz import PRODUCTS as CWtargs\n",
    "\n",
    "def get_conditional_rewards(frame,targets=CWtargs):\n",
    "    \"\"\"\n",
    "    Gives returns conditioned on the different targets\n",
    "    Inputs:\n",
    "        Frame (dataframe) - Pandas Dataframe containing gym information\n",
    "        targets (list) - List of N targets (reaction products)\n",
    "        \n",
    "    Outputs:\n",
    "        targets\n",
    "        rew (List<float>) - List of size N containing the average return given each target\n",
    "    \n",
    "    \"\"\"\n",
    "    # turn observation column into a numpy array\n",
    "    obs = np.stack(frame.InState)\n",
    "    N=len(targets)\n",
    "    rew=[]\n",
    "    for i in range(N):\n",
    "        #gather all data where the target is targets[N]\n",
    "        cframe=frame[obs[:,-N+i]>0.9]\n",
    "        #Obtain the mean reward of these episodes\n",
    "        rew+=[calc_return(cframe)]\n",
    "    return [targets,np.array(rew)]\n",
    "\n",
    "def get_conditional_actions(frame,targets=CWtargs):\n",
    "    \"\"\"\n",
    "    Gives actions conditioned on the different targets, meant for continuous action spaces\n",
    "    Inputs:\n",
    "        Frame (dataframe) - Pandas Dataframe containing gym information\n",
    "        targets (list) - List of N targets (reaction products)\n",
    "        \n",
    "    Outputs:\n",
    "        targets\n",
    "        act (List<array>) - List of size N containing the mean action given each target\n",
    "    \n",
    "    \"\"\"\n",
    "    # turn observation column into a numpy array\n",
    "    obs = np.stack(frame.InState)\n",
    "    N=len(targets)\n",
    "    act=[]\n",
    "    for i in range(N):\n",
    "        #gather all data where the target is targets[N]\n",
    "        cframe=frame[obs[:,-N+i]>0.9]\n",
    "        #Obtain the mean action of these episodes\n",
    "        act+=[cframe.Action.mean()]\n",
    "    return [targets,act]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f49cf0",
   "metadata": {},
   "source": [
    "# General Wurtz React:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda1f430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "folders = load_rollouts(\"GenWurtzReact-v1\",obj=default_obj,last=False)\n",
    "models=[a for a in folders]\n",
    "\n",
    "gppo = [folders[model] for model in models]\n",
    "gheuristic = pd.read_pickle(\"Legacy\\\\Legacy5\\\\MODELS\\\\GenWurtzReact-v1\\\\Heuristic/rollout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635a1a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "relative=True\n",
    "\n",
    "if relative:\n",
    "    ghr=get_conditional_rewards(gheuristic)[1]\n",
    "    info = ([get_conditional_rewards(gheuristic)[0]]+\n",
    "[(models[i], [get_conditional_rewards(gppo[i])[1]/ghr]) for i in range(len(models))]+\n",
    "[('Heuristic', [get_conditional_rewards(gheuristic)[1]])])\n",
    "\n",
    "else:\n",
    "    info = ([get_conditional_rewards(gheuristic)[0]]+\n",
    "[(models[i], [get_conditional_rewards(gppo[i])[1]]) for i in range(len(models))]+\n",
    "[('Heuristic', [get_conditional_rewards(gheuristic)[1]])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199f5649",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RadarGraph import *\n",
    "\n",
    "\n",
    "theta = radar_factory(len(info[0]), frame='polygon')\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(22, 7), nrows=1, ncols=len(models)+1,subplot_kw=dict(projection='radar'))\n",
    "fig.subplots_adjust(wspace=0.5, hspace=0.25, top=0.85, bottom=0.05)\n",
    "\n",
    "make_radar(theta,axs,info,colors = \"r\",gridlines=[0.0,0.4,0.8,1.2,1.6,2.0])\n",
    "\n",
    "labels = ('Return', '-', '1', 'Factor 4', 'Factor 5')\n",
    "legend = axs[0].legend(labels, loc=(0.9, .95),labelspacing=0.1, fontsize='small')\n",
    "\n",
    "\n",
    "fig.text(0.5, 0.8, \"Average Return VS Target Material (Best Run trained with 0.5M Steps)\",\n",
    "             horizontalalignment='center', color='black', weight='bold',\n",
    "             size='large')\n",
    "\n",
    "if relative:\n",
    "    #scale all but the heurstic the same\n",
    "    for ax in axs.flat[:-1]:\n",
    "        ax.set_rmin(min([a[0].min() for (c,a) in info[1:-1]]+[0]))\n",
    "        ax.set_rmax(max([a[0].max() for (c,a) in info[1:-1]]+[1]))\n",
    "else:\n",
    "    #scale them all the same\n",
    "    for ax in axs.flat:\n",
    "        ax.set_rmin(min([a[0].min() for (c,a) in info[1:]]+[0]))\n",
    "        ax.set_rmax(max([a[0].max() for (c,a) in info[1:]]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e69b899",
   "metadata": {},
   "outputs": [],
   "source": [
    "info0 = ([\n",
    "    ['dT', 'dV', '1-chlorohexane', '2-chlorohexane', '3-chlorohexane', 'Na' ]]+\n",
    "    [(models[i], [[b for b in act] for act in get_conditional_actions(gppo[i])[1]]) for i in range(len(models))]+\n",
    "    [('Heuristic', [[b for b in act] for act in get_conditional_actions(gheuristic)[1]])]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135ba709",
   "metadata": {},
   "source": [
    "# Mean Action Given a Target Material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcf3c9b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#for i in range(len(CWtargs)):\n",
    "i = 6\n",
    "\n",
    "info=[info0[0]]+[(md[0],md[1][i:i+1]) for md in info0[1:]]\n",
    "\n",
    "\n",
    "theta = radar_factory(len(info[0]), frame='polygon')\n",
    "fig, axs = plt.subplots(figsize=(22, 4), nrows=1, ncols=len(models)+1,subplot_kw=dict(projection='radar'))\n",
    "fig.subplots_adjust(wspace=0.5, hspace=0.25, top=0.85, bottom=0.05)\n",
    "c=['b', 'r', 'g', 'm', 'y',\"orange\",\"k\"][i:i+1]\n",
    "make_radar(theta,axs,info,colors=c)\n",
    "labels = [\"Target: \"+CWtargs[i]]\n",
    "legend = axs[0].legend(labels, loc=(0.9, .95),labelspacing=0.1, fontsize='small')\n",
    "fig.text(0.5, 1.0, \"Mean Value of Actions VS Target Material (Trained with 0.5M Steps)\",\n",
    "             horizontalalignment='center', color='black', weight='bold',\n",
    "             size='large')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c8a651",
   "metadata": {},
   "source": [
    "# Mean action at each timestep (Same Target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453119cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_act = ([\n",
    "actions_by_time(target_subset(gp,len(CWtargs),i)) for gp in gppo]\n",
    "+[actions_by_time(target_subset(gheuristic,len(CWtargs),i))])\n",
    "actions = ['dT', 'dV', '1-chlorohexane', '2-chlorohexane', '3-chlorohexane', 'Na' ]\n",
    "colors = [\"r\",\"g\",\"b\",\"c\",\"y\",\"m\"]\n",
    "names=models+[\"Heuristic\"]\n",
    "\n",
    "plotabt(all_act,actions,colors,names,title_ext=\"(GenWurtzReact Targeting %s)\"%CWtargs[i])\n",
    "\n",
    "plt.savefig(f\"Legacy/Figures/WurtzReact/Target-{CWtargs[i]}.pdf\",bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb18a962",
   "metadata": {},
   "source": [
    "# Fict React:\n",
    "\n",
    "Reactions at play:\n",
    "\n",
    "A+B $\\rightarrow$ E</br>\n",
    "A+D $\\rightarrow$ F</br>\n",
    "B+D $\\rightarrow$ G</br>\n",
    "F+G $\\rightarrow$ I</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b430979",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from chemistrylab.reactions.available_reactions.fict_react2 import PRODUCTS as FRtargs\n",
    "from chemistrylab.reactions.available_reactions.fict_react2 import REACTANTS as FRchoices\n",
    "\n",
    "obj = lambda x: get_conditional_rewards(x,FRtargs)[1].mean()\n",
    "\n",
    "#obj = lambda x: -get_conditional_rewards(x,FRtargs)[1].mean()\n",
    "\n",
    "folders = load_rollouts(\"FictReact-v2\",obj=obj,last=False,verbose=True)#\\\\PPO\\\\09-03-2023--15-14-59\",last=False)\n",
    "#folders = dict(PPO=\"MODELS/FictReact-v2/PPO\\\\09-03-2023--15-14-59\\\\rollout\",\n",
    "              #SAC=\"MODELS/FictReact-v2/SAC\\\\02-03-2023--15-55-48\\\\best_rollout\",\n",
    "              #A2C=\"MODELS/FictReact-v2/A2C\\\\02-03-2023--16-56-52\\\\best_rollout\",\n",
    "              #TD3=\"MODELS/FictReact-v2/TD3\\\\02-03-2023--18-04-00\\\\best_rollout\")\n",
    "\n",
    "models=[a for a in folders if a!=\"Heuristic\"]\n",
    "\n",
    "fppo = [folders[model] for model in models]\n",
    "\n",
    "fheuristic = pd.read_pickle(\"MODELS/FictReact-v2/Heuristic/rollout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221f53e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "relative=True\n",
    "\n",
    "if relative:\n",
    "    fhr=get_conditional_rewards(fheuristic,FRtargs)[1]\n",
    "    info = ([get_conditional_rewards(fheuristic,FRtargs)[0]]+\n",
    "[(models[i], [get_conditional_rewards(fppo[i],FRtargs)[1]/fhr]) for i in range(len(models))]+\n",
    "[('Heuristic', [get_conditional_rewards(fheuristic,FRtargs)[1]])])\n",
    "\n",
    "else:\n",
    "    info = ([get_conditional_rewards(fheuristic,FRtargs)[0]]+\n",
    "[(models[i], [get_conditional_rewards(fppo[i],FRtargs)[1]]) for i in range(len(models))]+\n",
    "[('Heuristic', [get_conditional_rewards(fheuristic,FRtargs)[1]])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42189af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = radar_factory(len(info[0]), frame='polygon')\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(22, 7), nrows=1, ncols=len(models)+1,subplot_kw=dict(projection='radar'))\n",
    "\n",
    "#axs = np.array([axs])\n",
    "\n",
    "fig.subplots_adjust(wspace=0.5, hspace=0.25, top=0.85, bottom=0.05)\n",
    "\n",
    "make_radar(theta,axs,info,colors = \"r\",gridlines=[0.0,0.2,0.4,0.6,0.8])\n",
    "\n",
    "labels = ('Return', '-', '1', 'Factor 4', 'Factor 5')\n",
    "legend = axs[0].legend(labels, loc=(0.9, .95),labelspacing=0.1, fontsize='small')\n",
    "\n",
    "\n",
    "fig.text(0.5, 0.8, \"Average Return VS Target Material (Best model trained with 0.5M Steps)\",\n",
    "             horizontalalignment='center', color='black', weight='bold',\n",
    "             size='large')\n",
    "\n",
    "if relative:\n",
    "    #scale all but the heurstic the same\n",
    "    for ax in axs.flat[:-1]:\n",
    "        ax.set_rmin(max(0,min([a[0].min() for (c,a) in info[1:-1]]+[0])))\n",
    "        ax.set_rmax(max([a[0].max() for (c,a) in info[1:-1]]+[1]))\n",
    "else:\n",
    "    #scale them all the same\n",
    "    for ax in axs.flat:\n",
    "        ax.set_rmin(min([a[0].min() for (c,a) in info[1:]]+[0]))\n",
    "        ax.set_rmax(max([a[0].max() for (c,a) in info[1:]]))\n",
    "        \n",
    "#plt.savefig(\"Legacy/Figures/FictReactBest.pdf\",bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5f5423",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp=folders[\"TD3\"]\n",
    "\n",
    "obs = np.stack(tmp.InState)\n",
    "tmp[obs[:,-1]>0.9].shape[0]/20\n",
    "print(tmp[tmp.Done==True].Reward.mean(),calc_return(tmp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e47715",
   "metadata": {},
   "source": [
    "# Mean Action VS Target Material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e696d83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "info0 = ([\n",
    "    [\"dT\",\"dV\"]+FRchoices]+\n",
    "    [(models[i], [[b for b in act] for act in get_conditional_actions(fppo[i],FRtargs)[1]]) for i in range(len(models))]+\n",
    "    [('Heuristic', [[b for b in act] for act in get_conditional_actions(fheuristic,FRtargs)[1]])]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cbf586",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(len(FRtargs)):\n",
    "i=0\n",
    "info=[info0[0]]+[(md[0],md[1][i:i+1]) for md in info0[1:]]\n",
    "\n",
    "\n",
    "theta = radar_factory(len(info[0]), frame='polygon')\n",
    "fig, axs = plt.subplots(figsize=(22, 4), nrows=1, ncols=len(models)+1,subplot_kw=dict(projection='radar'))\n",
    "fig.subplots_adjust(wspace=0.5, hspace=0.25, top=0.85, bottom=0.05)\n",
    "c=['b', 'r', 'g', 'm', 'y',\"orange\",\"r\"][i:i+1]\n",
    "make_radar(theta,axs,info,colors=c)\n",
    "labels = [\"Target: \"+FRtargs[i]]\n",
    "legend = axs[0].legend(labels, loc=(0.9, .95),labelspacing=0.1, fontsize='small')\n",
    "fig.text(0.5, 1.0, \"Mean Value of Actions VS Target Material (Trained with 0.5M Steps)\",\n",
    "             horizontalalignment='center', color='black', weight='bold',\n",
    "             size='large')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b226a48",
   "metadata": {},
   "source": [
    "# ... At each Timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0577796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=3\n",
    "\n",
    "all_act = ([\n",
    "actions_by_time(target_subset(fp,len(FRtargs),i)) for fp in fppo]\n",
    "+[actions_by_time(target_subset(fheuristic,len(FRtargs),i))])\n",
    "colors = [\"r\",\"g\",\"b\",\"c\",\"y\",\"m\"]\n",
    "names=models+[\"Heuristic\"]\n",
    "\n",
    "plotabt(all_act,[\"dT\",\"dV\"]+FRchoices,colors,names,title_ext=\"(FictReact Targeting %s)\"%FRtargs[i])\n",
    "\n",
    "plt.savefig(f\"Legacy/Figures/FictReact/Target-{FRtargs[i]}.pdf\",bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bd4e26",
   "metadata": {},
   "source": [
    "# Handling Box-Discrete actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a993d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discrete_actions(frame,N=None,N2=None):\n",
    "    \"\"\"\n",
    "    Gives distribution of actions (index 0) taken as well as the average value of the actions at index 1\n",
    "    Inputs:\n",
    "        Frame (dataframe) - Pandas Dataframe containing gym information        \n",
    "    Outputs:\n",
    "        act0 (list<float>) - Action (index 0) distribution\n",
    "        act1 (list(float)) - Average action at index 1\n",
    "    \n",
    "    \"\"\"\n",
    "    # turn observation column into a numpy array\n",
    "    act = np.stack(frame.Action)    \n",
    "    if len(act.shape)<2:\n",
    "        act0=act\n",
    "        act=np.zeros(act0.shape+(2,),dtype=np.int32)\n",
    "        act[:,0]=act0//N2\n",
    "        act[:,1] = act0%N2\n",
    "        \n",
    "    if N is None:\n",
    "        N = np.max(act[:,0])\n",
    "    N0= np.max(act[:,1])\n",
    "    act0=[]\n",
    "    act1=[]\n",
    "    #print(N)\n",
    "    for i in range(N+1):\n",
    "        #gather all data where the target is targets[N]\n",
    "        cframe=act[act[:,0]==i]\n",
    "        \n",
    "        #print(cframe)\n",
    "        #Obtain the mean action of these episodes\n",
    "        act0+=[len(cframe)/act.shape[0]]\n",
    "        if len(cframe)==0:\n",
    "            act1+=[0]\n",
    "        else:\n",
    "            act1+=[cframe[:,1].mean()/N0]\n",
    "    return [act0,act1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6d37e01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_actions_by_time(frame,N=None,N2=None):\n",
    "    \"\"\"Gives the mean action at each timestep of your rollout dataframe when actions are discrete\n",
    "    \n",
    "    Inputs:\n",
    "        frame (dataframe) - Pandas Dataframe containing rollouts\n",
    "        N (int)           - Number of actions in MultiDiscrete dim 0\n",
    "        N2 (int)          - Number of actions in MultiDiscrete dim 1\n",
    "    \n",
    "    \"\"\"\n",
    "    min_t,max_t = frame.Step.min(),frame.Step.max()\n",
    "    mean_act=[]\n",
    "    \n",
    "    act = np.stack(frame.Action)  \n",
    "\n",
    "    \n",
    "    if N is None:\n",
    "        N = np.max(act[:,0])\n",
    "    \n",
    "    for t in range(min_t,max_t+1):\n",
    "        act = np.stack(frame.Action[frame.Step==t])\n",
    "        if len(act.shape)<2:\n",
    "            act0=act\n",
    "            act=np.zeros(act0.shape+(2,),dtype=np.int32)\n",
    "            act[:,0]=act0//N2\n",
    "            act[:,1] = act0%N2\n",
    "        mean_act_t =np.zeros(N+1)\n",
    "        for i in range(N+1):\n",
    "            mean_act_t[i] = (act[:,0]==i).mean()\n",
    "        mean_act+=[mean_act_t]\n",
    "    return np.array(mean_act)\n",
    "\n",
    "\n",
    "def hashed_trajectories(frame,N=None,N2=None):\n",
    "    \"\"\"Turns rollouts into a dictionary of trajectories and counts by hashing episodes based on the actions taken.\n",
    "    \n",
    "    Inputs:\n",
    "        frame (dataframe) - Pandas Dataframe containing rollouts\n",
    "        N (int)           - Number of actions in MultiDiscrete dim 0\n",
    "        N2 (int)          - Number of actions in MultiDiscrete dim 1\n",
    "    \n",
    "    Outputs:\n",
    "        trajectories (dict) - String representations of actions are keys and the number appearances are values\n",
    "    \n",
    "    Example: \n",
    "    \n",
    "    >>> print(frame) \n",
    "    >>>         InState  Action  Reward  OutState   Done Info  Step\n",
    "            0  0.631918  [0, 9]     0.0  0.632225  False   {}     0\n",
    "            1  0.632225  [0, 9]     0.0    0.6319  False   {}     1\n",
    "            2    0.6319  [4, 4]     0.8    0.6319   True   {}     2\n",
    "            \n",
    "    >>> hashed_trajectories(frame)\n",
    "    >>> {'090944': 1}\n",
    "    \n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    min_t,max_t = frame.Step.min(),frame.Step.max()\n",
    "    mean_act=[]\n",
    "    \n",
    "    act = np.stack(frame.Action)  \n",
    "\n",
    "    trajectories=dict()\n",
    "    \n",
    "    if N is None:\n",
    "        N = np.max(act[:,0])\n",
    "    act_string=\"\"\n",
    "    for t,act in enumerate(frame.Action):\n",
    "        if len(act.shape)<1:\n",
    "            act0=int(act)\n",
    "            act=np.zeros((2,),dtype=np.int32)\n",
    "            act[0]=act0//N2\n",
    "            act[1] = act0%N2\n",
    "        act_string+=(str(act[0])+str(act[1]))\n",
    "        \n",
    "        if frame.Done[t]:\n",
    "            trajectories[act_string] = trajectories.get(act_string,0)+1\n",
    "            act_string=\"\"\n",
    "            \n",
    "    return trajectories\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "766dbe3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    InState  Action  Reward  OutState   Done Info  Step\n",
      "0  0.631918  [0, 9]     0.0  0.632225  False   {}     0\n",
      "1  0.632225  [0, 9]     0.0    0.6319  False   {}     1\n",
      "2    0.6319  [4, 4]     0.8    0.6319   True   {}     2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sprag\\AppData\\Local\\Temp\\ipykernel_3720\\4294213671.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  example.InState[i]=s.mean()\n",
      "C:\\Users\\sprag\\AppData\\Local\\Temp\\ipykernel_3720\\4294213671.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  example.OutState[i]=s.mean()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'090944': 1}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = pd.read_pickle(\"Legacy/Legacy5/MODELS\\\\WurtzDistill-v1\\\\Heuristic\\\\rollout\")[:3]\n",
    "for i,s in enumerate(example.InState):\n",
    "    example.InState[i]=s.mean()\n",
    "for i,s in enumerate(example.OutState):\n",
    "    example.OutState[i]=s.mean()\n",
    "    \n",
    "print(example)\n",
    "hashed_trajectories(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e504cd",
   "metadata": {},
   "source": [
    "# Distillation Bench Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5347cca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = load_rollouts(\"WurtzDistill-v1\",obj=None,last=False,verbose=False)\n",
    "\n",
    "#folders.update(load_rollouts(\"DiscreteWurtzDistill-v1\",obj=default_obj,last=True))\n",
    "\n",
    "models = [a for a in folders]\n",
    "dppo = [folders[model] for model in models]\n",
    "dheuristic = pd.read_pickle(\"Legacy/Legacy5/MODELS\\\\WurtzDistill-v1\\\\Heuristic\\\\rollout\")\n",
    "#dheuristic = pd.read_pickle(\"FR2H/rollout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a553c08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = ([['dT', 'Pour 0->1', 'Pour 1->2', 'Wait', 'End Experiment' ]]+\n",
    "[(models[i], get_discrete_actions(dppo[i],N=4,N2=10)[::-1]+\n",
    " [[default_obj(dppo[i])]*5]\n",
    " ) for i in range(len(models))]+\n",
    "[('Heuristic', get_discrete_actions(dheuristic,N=4)[::-1]\n",
    "+[[dheuristic[dheuristic.Done==True].Reward.mean()]*5])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d2e21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = radar_factory(len(info[0]), frame='polygon')\n",
    "fig, axs = plt.subplots(figsize=(22, 4), nrows=1, ncols=len(models)+1,subplot_kw=dict(projection='radar'))\n",
    "fig.subplots_adjust(wspace=0.5, hspace=0.25, top=0.85, bottom=0.05)\n",
    "c=['b', 'r', 'g', (0,0.1,0), 'y',\"orange\",\"r\"]#[i:i+1]\n",
    "make_radar(theta,axs,info,colors=c)\n",
    "labels = ( 'Action average sub-value','Percent Action Taken',\"Average Return\")\n",
    "legend = axs[0].legend(labels, loc=(0.9, .95),labelspacing=0.1, fontsize='small')\n",
    "fig.text(0.5, 1.0, \"Comparison of the frequency of actions taken (Trained with 50K Steps)\",\n",
    "             horizontalalignment='center', color='black', weight='bold',\n",
    "             size='large')\n",
    "\n",
    "\n",
    "\n",
    "#scale them all the same\n",
    "for ax in axs:\n",
    "    ax.set_rmin(0)\n",
    "    ax.set_rmax(1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b1ed2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    all_act = ([\n",
    "    discrete_actions_by_time(dp,N=4,N2=10) for dp in dppo]\n",
    "    +[discrete_actions_by_time(dheuristic,N=4,N2=10)])\n",
    "    colors = [\"r\",\"g\",\"b\",\"c\",\"y\",\"m\"]\n",
    "    names=models+[\"Heuristic\"]\n",
    "    actions=['dT', 'Pour 0->1', 'Pour 1->2', 'Wait', 'End Experiment' ]\n",
    "    plotabt(all_act,actions,colors,names,title=\"Frequency of each action type VS step (WurtzDistill)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74c38ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distill_relabel(trajectory):\n",
    "    \n",
    "    B1_full=False\n",
    "    \n",
    "    pouring_actions = {\"1\",\"2\"}\n",
    "    \n",
    "    wait_string = \"31\"\n",
    "    \n",
    "    new_trajectory=\"\"\n",
    "    \n",
    "    for action,param in zip(trajectory[::2],trajectory[1::2]):\n",
    "        \n",
    "        #standardize the end experiment actions\n",
    "        if action == \"4\":param=\"5\"\n",
    "        \n",
    "        \n",
    "        if param==\"0\":\n",
    "            #Pouring zero amount is the same as waiting\n",
    "            if action in pouring_actions:\n",
    "                new_trajectory+=wait_string\n",
    "            else:\n",
    "                new_trajectory+=action+param\n",
    "        else:\n",
    "            if action == \"1\": #Pouring 0->1\n",
    "                B1_full=True\n",
    "                new_trajectory+=action+param\n",
    "            if action == \"0\": #Potentially Boiling from 0->1\n",
    "                B1_full=True\n",
    "                new_trajectory+=action+param\n",
    "            elif action==\"2\" and not B1_full: #Pouring from 1->2 won't do anything\n",
    "                #draining stuff from empty B2\n",
    "                new_trajectory+=wait_string\n",
    "            else:\n",
    "                new_trajectory+=action+param\n",
    "    return new_trajectory\n",
    "\n",
    "def distill_redo_dict(trajectories):\n",
    "    new_traj=dict()\n",
    "    for key in trajectories:\n",
    "        key2 = distill_relabel(key)\n",
    "        new_traj[key2]=new_traj.get(key2,0)+trajectories[key]\n",
    "    return new_traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c9e9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = load_rollouts(\"WurtzDistill-v1\",obj=None,last=False,TOL=1e-2)\n",
    "models = [a for a in folders]\n",
    "dppo = [folders[model] for model in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94576dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "actions = ['dT', 'Pour 0->1', 'Pour 1->2', 'Wait', 'End Experiment' ]\n",
    "toact = lambda x: \"|\".join([actions[int(i)]+\":\"+x[2*j+1] for j,i in enumerate(x[::2])])\n",
    "tograph = lambda x: [int(i)*10+int(x[2*j+1]) for j,i in enumerate(x[::2])]\n",
    "\n",
    "i=0\n",
    "print(models[i])\n",
    "hashes = hashed_trajectories(dppo[i],N=4,N2=10)\n",
    "\n",
    "hashes=distill_redo_dict(hashes)\n",
    "\n",
    "#hashes={\"090945\":1}\n",
    "\n",
    "sorted_hashes = sorted([a for a in hashes],key=lambda x:hashes[x],reverse=True)\n",
    "sorted_amounts = [hashes[x] for x in sorted_hashes]\n",
    "\n",
    "L=max(12,len(sorted_hashes[0])//2)\n",
    "\n",
    "print(sorted_hashes[0],sorted_amounts[0],sum(sorted_amounts))\n",
    "\n",
    "toact(sorted_hashes[0])\n",
    "\n",
    "tograph = lambda x: [int(i)*10+float(x[2*j+1])*0.78+(9-9*.78)/2 for j,i in enumerate(x[::2])]\n",
    "\n",
    "fig = plt.figure(1,figsize=(7,3), dpi=240, facecolor='w', edgecolor='k')\n",
    "\n",
    "for j,act in enumerate(actions[::-1]):\n",
    "    j=len(actions)-j-1\n",
    "    plt.fill_between([-0.5,L],[j*10-0.5,j*10-0.5],[j*10+9.5,j*10+9.5],label=act,alpha=0.5)\n",
    "    plt.text(L*0.9875,j*10+5,act,horizontalalignment=\"right\",bbox=dict(boxstyle=\"square\",facecolor=\"w\",edgecolor=\"k\",alpha=0.2))\n",
    "    \n",
    "for a,string in enumerate(sorted_hashes):\n",
    "    if len(string)<26 or a<10:\n",
    "        plt.plot(tograph(string),\"k.-\",ms=5,alpha=(sorted_amounts[a]/sorted_amounts[0])**1)\n",
    "\n",
    "        \n",
    "\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Action\")\n",
    "plt.title(models[i]+ \" Most Common Trajectories (Distillation Bench: All 10 runs, Best Checkpoint)\")\n",
    "\n",
    "plt.xlim(-0.5,L)\n",
    "plt.ylim(-0.5,49.5)\n",
    "plt.yticks([1,4.5,8,11,18,21,28,31,38],[-1,0,1,0,1,0,1,0,1])\n",
    "\n",
    "\n",
    "#plt.savefig(f\"Legacy/figures/Distillation/{models[i]}-{sorted_hashes[0][:20]}-{sorted_amounts[0]}.pdf\",bbox_inches='tight')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "#plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab4ef74",
   "metadata": {},
   "source": [
    "# Extraction Bench Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9b7eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#folders = load_rollouts(\"Legacy\\\\Legacy5\\\\MODELS\\\\DiscreteWurtzExtract-v1\",obj=default_obj,last=False,TOL=0)\n",
    "\n",
    "folders = load_rollouts(\"MODELS\\\\DiscreteWurtzExtract-v1\",obj=default_obj,last=False,TOL=0,verbose=False)\n",
    "\n",
    "folders.update(load_rollouts(\"Legacy\\\\Legacy5\\\\MODELS\\\\DiscreteWurtzExtract-v1\\\\PPO-XL\",obj=default_obj,last=False,TOL=0))\n",
    "\n",
    "models = [a for a in folders]\n",
    "eppo = [folders[model] for model in models]\n",
    "\n",
    "eheuristic = pd.read_pickle(\"Legacy/Legacy5/MODELS\\\\WurtzExtract-v1\\\\Heuristic\\\\rollout\")\n",
    "\n",
    "#action_set = ['Draining from ExV to Beaker1', 'Mix ExV', \"Mix B1\", \"Mix B2\", \"Pour from B1 to ExV\", \"Pour from B1 to B2\",\n",
    "#              'Pour from ExV to B2', 'Add oil, pour from Oil Vessel to ExV', 'wait', 'Done']\n",
    "\n",
    "action_set=[\"Drain EV to B1\", \"Mix EV\",\"Pour B1 into EV\",\"Pour B2 into EV\", \n",
    "            \"Pour EV into B2\", \"Pour S1 into EV\", \"Pour S2 into EV\",\"End Experiment\"]\n",
    "\n",
    "\n",
    "N = len(action_set)-1\n",
    "info = ([action_set]+\n",
    "[(models[i], get_discrete_actions(eppo[i],N,5)[::-1]+\n",
    " [[eppo[i][eppo[i].Done==True].Reward.mean()]*(N+1)]\n",
    " ) for i in range(len(models))]+\n",
    "[('Heuristic', get_discrete_actions(eheuristic,N)[::-1]\n",
    "+[[eheuristic[eheuristic.Done==True].Reward.mean()]*(N+1)])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48005720",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "theta = radar_factory(len(info[0]), frame='polygon')\n",
    "fig, axs = plt.subplots(figsize=(22, 4), nrows=1, ncols=len(models)+1,subplot_kw=dict(projection='radar'))\n",
    "fig.subplots_adjust(wspace=0.5, hspace=0.25, top=0.85, bottom=0.05)\n",
    "c=['b', 'r', 'g', 'm', 'y',\"orange\",\"r\"]#[i:i+1]\n",
    "make_radar(theta,axs,info,colors=c)\n",
    "labels = ( 'Action average sub-value','Percent Action Taken',\"Average Return\")\n",
    "legend = axs[0].legend(labels, loc=(0.9, .95),labelspacing=0.1, fontsize='small')\n",
    "fig.text(0.5, 1.0, \"Comparison of the frequency of actions taken (Trained with 2M Steps)\",\n",
    "             horizontalalignment='center', color='black', weight='bold',\n",
    "             size='large')\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set_rmin(min([np.array(a).min() for (c,a) in info[1:]]+[0]))\n",
    "    ax.set_rmax(max([np.array(a).max() for (c,a) in info[1:]]+[1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c7b5a5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "actions=[\"Drain EV to B1\", \"Mix EV\",\"Pour B1 into EV\",\"Pour B2 into EV\", \n",
    "            \"Pour EV into B2\", \"Pour S1 into EV\", \"Pour S2 into EV\",\"End Experiment\"]\n",
    "N = len(actions)-1\n",
    "\n",
    "all_act = ([\n",
    "discrete_actions_by_time(ep,N,5) for ep in eppo]\n",
    "+[discrete_actions_by_time(eheuristic,N,5)])\n",
    "colors = [\"r\",\"g\",\"b\",\"c\",\"y\",\"m\"]+[\"k\"]*10\n",
    "names=models+[\"Heuristic\"]\n",
    "\n",
    "plotabt(all_act,actions,colors,names,title=\"(Normalized) Number of times Each Action was taken at each Step (WurtzExtract)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2a219d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relabel(trajectory):\n",
    "    B2_full=False\n",
    "    \n",
    "    B1_full=False\n",
    "    \n",
    "    pouring_actions = {\"0\",\"2\",\"3\",\"4\",\"5\",\"6\"}\n",
    "    \n",
    "    wait_string = \"82\"\n",
    "    \n",
    "    new_trajectory=\"\"\n",
    "    \n",
    "    for action,param in zip(trajectory[::2],trajectory[1::2]):\n",
    "        \n",
    "        #standardize the end experiment actions\n",
    "        if action==\"7\":param=\"2\"\n",
    "        \n",
    "        if param==\"0\":\n",
    "            #Pouring zero amount is the same as waiting\n",
    "            if action in pouring_actions:\n",
    "                new_trajectory+=wait_string\n",
    "            else:\n",
    "                new_trajectory+=action+param\n",
    "        else:\n",
    "            if action == \"4\": #Pouring stuff into B2\n",
    "                B2_full=True\n",
    "                new_trajectory+=action+param\n",
    "            elif action==\"3\" and not B2_full:\n",
    "                #draining stuff from empty B2\n",
    "                new_trajectory+=wait_string\n",
    "            elif action == \"0\": #Pouring stuff into B1\n",
    "                B1_full=True\n",
    "                new_trajectory+=action+param\n",
    "            elif action==\"2\" and not B1_full:\n",
    "                #draining stuff from empty B1\n",
    "                new_trajectory+=wait_string\n",
    "            else:\n",
    "                new_trajectory+=action+param\n",
    "    return new_trajectory\n",
    "\n",
    "def extract_redo_dict(trajectories):\n",
    "    new_traj=dict()\n",
    "    for key in trajectories:\n",
    "        key2 = extract_relabel(key)\n",
    "        new_traj[key2]=new_traj.get(key2,0)+trajectories[key]\n",
    "    return new_traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c9fce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions=[\"Drain EV to B1\", \"Mix EV\",\"Pour B1 into EV\",\"Pour B2 into EV\", \n",
    "            \"Pour EV into B2\", \"Pour S1 into EV\", \"Pour S2 into EV\",\"End Experiment\",\"Wait\"]\n",
    "\n",
    "toact = lambda x: \"|\".join([actions[int(i)]+\":\"+x[2*j+1] for j,i in enumerate(x[::2])])\n",
    "\n",
    "i=3\n",
    "print(models[i])\n",
    "hashes = hashed_trajectories(eppo[i],N=8,N2=5)\n",
    "\n",
    "hashes = {\"518282820104040404\"+\"5161828282010404\"+\"41618282820304043472\":1}\n",
    "\n",
    "hashes=extract_redo_dict(hashes)\n",
    "\n",
    "sorted_hashes = sorted([a for a in hashes],key=lambda x:hashes[x],reverse=True)\n",
    "sorted_amounts = [hashes[x] for x in sorted_hashes]\n",
    "\n",
    "\n",
    "print(sorted_hashes[0],sorted_amounts[0],sum(sorted_amounts))\n",
    "\n",
    "fig = plt.figure(1,figsize=(7,4), dpi=240, facecolor='w', edgecolor='k')\n",
    "\n",
    "tograph = lambda x: [int(i)*10+float(x[2*j+1])*1.58+(9-9*.78)/2 for j,i in enumerate(x[::2])]\n",
    "\n",
    "L=max(8,len(sorted_hashes[0])//2+4)\n",
    "\n",
    "for j,act in enumerate(actions[::-1]):\n",
    "    j=len(actions)-j-1\n",
    "    plt.fill_between([-0.5,L],[j*10-0.5,j*10-0.5],[j*10+9.5,j*10+9.5],label=act,alpha=0.5)\n",
    "    plt.text(L*0.9875,j*10+5,act,horizontalalignment=\"right\",bbox=dict(boxstyle=\"square\",facecolor=\"w\",edgecolor=\"k\",alpha=0.2))\n",
    "    \n",
    "for a,string in enumerate(sorted_hashes[:1]):\n",
    "    #string = extract_relabel(string)\n",
    "    if len(string)<40 or a<10:\n",
    "        plt.plot(tograph(string),\"k.-\",ms=5,alpha=(sorted_amounts[a]/sorted_amounts[0])**1.0)\n",
    "\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Action\")\n",
    "#plt.title(models[i]+ \" Most Common Trajectories (Extraction Bench: Best Checkpoint of 10 runs)\")\n",
    "\n",
    "plt.title(\"Extract Policy (0.59 reward)\")\n",
    "\n",
    "#plt.title(\"91% purity steps\")\n",
    "plt.xlim(-0.5,L)\n",
    "plt.ylim(-0.5,89.5)\n",
    "plt.yticks([1,8,11,18,21,28,31,38,41,48,51,58,61,68],[0,1,0,1,0,1,0,1,0,1,0,1,0,1])\n",
    "#plt.yticks([])\n",
    "#plt.savefig(f\"Legacy/figures/DiscreteExtraction/{models[i]}-{sorted_hashes[0][:20]}-{sorted_amounts[0]}.pdf\",bbox_inches='tight')\n",
    "\n",
    "plt.savefig(f\"Legacy/figures/DiscreteExtraction/Heuristic.pdf\",bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "#plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c7f906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6ac85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = eppo[0]\n",
    "\n",
    "frame.Action[frame.Step==0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2bad97",
   "metadata": {},
   "outputs": [],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2076ec8",
   "metadata": {},
   "source": [
    "# React Bench Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fca714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for algo in [\"SAC\",\"A2C\",\"PPO\",\"TD3\"]:\n",
    "    for seed in [101,201,301]:\n",
    "        os.system(\"python RLTrain.py algorithm=%s seed=%d environment=GenWurtzReact-v1 steps=50000 n_envs=10 best_ratio=0.0 best_episodes=1\"%(algo,seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dbb3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for algo in [\"PPO\",\"SAC\",\"A2C\",\"TD3\"]:\n",
    "        for seed in [101,201,301]:\n",
    "            os.system(\"python RLTrain.py algorithm=%s seed=%d environment=FictReact-v2 steps=50000 n_envs=10 best_ratio=0.0 best_episodes=1\"%(algo,seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628715e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "            os.system(\"python RLTrain.py algorithm=%s seed=%d environment=FictReact-v2 steps=50000 n_envs=10 best_ratio=0.0 best_episodes=1\"%(algo,seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e37ee20",
   "metadata": {},
   "source": [
    "# Extraction and Distillation Bench Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8062cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "for seed in [101,201,301]:\n",
    "    os.system(\"python RLTrain.py algorithm=DQN seed=%d environment=DiscreteWurtzExtract-v1 steps=50000 n_envs=10 best_ratio=0.5 best_episodes=200\"%(seed))\n",
    "    \n",
    "for seed in [401,501,601]:\n",
    "    os.system(\"python RLTrain.py algorithm=DQN seed=%d environment=DiscreteWurtzExtract-v1 steps=200000 n_envs=10 best_ratio=0.5 best_episodes=200\"%(seed))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e436e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for seed in [101,201,301]:\n",
    "    os.system(\"python RLTrain.py algorithm=DQN seed=%d environment=DiscreteWurtzDistill-v1 steps=50000 n_envs=10 best_ratio=1e-5 best_episodes=1\"%(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513a4541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#from RLTrain import ALGO\n",
    "#print (ALGO)\n",
    "for algo in ['A2C']:#,'PPO'\n",
    "    for seed in [301]:\n",
    "        os.system(\"python RLTrain.py algorithm=%s seed=%d environment=WurtzExtract-v1 steps=50000 n_envs=10 best_ratio=0.0 best_episodes=1\"%(algo,seed))\n",
    "        os.system(\"python RLTrain.py algorithm=%s seed=%d environment=WurtzDistill-v1 steps=50000 n_envs=10 best_ratio=0.0 best_episodes=1\"%(algo,seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8cd3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for algo in ['PPO', 'A2C']:\n",
    "    os.system(\"python RLTrain.py algorithm=%s environment=WurtzExtract-v1 steps=50000\"%algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abcd183",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = \"A2C\"\n",
    "os.system(\"python RLTrain.py algorithm=%s environment=WurtzDistill-v1 steps=50000\"%algo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f39355",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9cacf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "for a,b,c in os.walk(\"./MODELS\"):\n",
    "    #print(a)\n",
    "    if \"-v\" in a and \"2023\" in a:\n",
    "        print(a[2:])\n",
    "        os.system(\"python RLTest.py %s steps=5000\"%a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0403c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"python RLTest.py WDH environment=WurtzDistill-v1 algorithm=WDH steps=5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9550db",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"python RLTest.py MODELS\\\\GenWurtzReact-v1\\\\Heuristic environment=GenWurtzReact-v1 algorithm=WRH steps=5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3f7ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in [\"PPO_WurtzExtract-v1\",\"A2C_WurtzExtract-v1\"]:\n",
    "    os.system(\"python RLTest.py %s steps=5000\"%a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6209bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system(\"python RLTest.py MODELS\\\\WurtzReact-v1\\\\Heuristic environment=WurtzReact-v1 algorithm=WRH steps=5000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ec92e1",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "\n",
    "Check this out https://github.com/yuanmingqi/rl-exploration-baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a425a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system(\"python RLTest.py PPO_RE3_WurtzExtract-v1 steps=5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcc4820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for a,b,c in os.walk(\"./\"):\n",
    "    #print(a)\n",
    "    if \"2023\" in a and not \"Legacy\" in a:\n",
    "         os.system(\"python RLTest.py %s steps=5000 --best\"%a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e76fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for seed in range(1,4):\n",
    "    os.system(\"python RLTrain.py environment=DiscreteWurtzExtract-v1 algorithm=DQN n_envs=10 steps=200000 best_episodes=200 best_ratio=0.5 seed=%d01\"%seed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
