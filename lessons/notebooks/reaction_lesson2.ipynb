{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reaction Bench Lesson 2\n",
    "\n",
    "### Getting a high reward in a reaction of form: \n",
    "\n",
    "A-X + A-X -> A-A + X-X and A-X + B-X -> A-B + X-X\n",
    "\n",
    "In this lesson we will try to get a high reward in the reaction of the form above. Rewards will come from producing either A-A or B-B. It's important to note that the reward cannot come from A-B as this doesn't make the desired property. The reaction we will be taking an in depth look at in this lesson is:\n",
    "\n",
    "2 3-chlorohexane + 2 Na -> 4,5-diethyloctane + 2 NaCl\n",
    "\n",
    "We will try to get the desired material: 4,5-diethyloctane\n",
    "\n",
    "In similar fashion to lesson 1, the reactions used in this lesson are found in the available reactions file. This particular lesson will use the reaction file `chloro_wurtz_v1.py` and is registered under the id `WurtzReact-v2`\n",
    "\n",
    "From lesson 1 we know that our action space is a 6 element vector represented by:\n",
    "\n",
    "|              | Temperature | Volume | 1-chlorohexane | 2 chlorohexane | 3-chlorohexane | Na  |\n",
    "|--------------|-------------|--------|----------------|----------------|----------------|-----|\n",
    "| Value range: | 0-1         | 0-1    | 0-1            | 0-1            | 0-1            |0-1|\n",
    "\n",
    "Each index corresponds to the following label and how we change them. For example is action[0] = 0 then the temperature will decrease by dt.\n",
    "If it is set to 0.5 then it will stay the same and if set to 1 then the temperature will increase by dt.\n",
    "\n",
    "First let's start by importing all the modules we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the required external modules\n",
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "from time import sleep\n",
    "from gym import envs\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "# ensure all necessary modules can be found\n",
    "sys.path.append(\"../\") # to access chemistrylab\n",
    "sys.path.append(\"../chemistrylab/reactions\") # to access all reactions\n",
    "\n",
    "# import all local modules\n",
    "import chemistrylab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will show all the environments we can currently run. Eventually you can create your own environments with different reactions and target material using the reaction template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all environments for reaction bench\n",
    "all_envs = envs.registry.all()\n",
    "env_ids = [env_spec.id for env_spec in all_envs if 'React' in env_spec.id]\n",
    "print(env_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This explains the simulated reaction we are trying to simulate and is initializing the reaction environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to get high reward for wurtz reaction of form:\n",
    "# A-X + A-X --> A-A + X-X and A-X + B-X --> A-B + X-X\n",
    "# Rewards comes from producing A-A or B-B\n",
    "# Cannot come from A-B as this doesn't make the desired property\n",
    "# Desired material in this case is initialized to be 4,5-diethyloctane\n",
    "# initializes environment\n",
    "env = gym.make(\"WurtzReact-v2\")\n",
    "render_mode = \"human\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "__ = env.reset()\n",
    "\n",
    "# shows # of actions available\n",
    "print('# of actions available: ',env.action_space.shape[0])\n",
    "num_actions_available = env.action_space.shape[0]\n",
    "\n",
    "total_steps=0\n",
    "total_reward=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will store certain values in these arrays so can plot them later on to visually show how each variable changes over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_over_time=[]\n",
    "steps_over_time=[]\n",
    "reactant_1 = []\n",
    "reactant_2 = []\n",
    "total_reward_over_time = []\n",
    "\n",
    "action = np.ones(env.action_space.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key to achieving a high reward in this simulation is to only add the reactants that are needed for the reaction to \n",
    "continue. This means that we will only add 3-chlorohexane and Na with our actions. This will allow us to maximize our \n",
    "reward as a large quantity of these reactants means the reaction with our target material will occur more often. We \n",
    "do this by running the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if total_steps  < 20:\n",
    "    action[0] = 1    # temperature\n",
    "    action[1] = 1    # volume\n",
    "    action[2] = 0    # 1-chlorohexane\n",
    "    action[3] = 0    # 2-chlorohexane\n",
    "    action[4] = 1    # 3-chlorohexane\n",
    "    action[5] = 1    # Na"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we're only adding the reactants we need for the reaction to continue; 3-chlorohexane\n",
    "\n",
    "![reaction](../../docs/sample_figures/reaction.png)\n",
    "\n",
    "<a style=\"font-size: 10px\">(source https://pixabay.com/vectors/chemical-reaction-experiment-flask-24562/)</a>\n",
    "\n",
    "Let's run our program and see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while not done:\n",
    "    # Actions:\n",
    "    #   a[0] changes the temperature between -dT (a[0] = 0.0) and +dT (a[0] = 1.0)\n",
    "    #   a[1] changes the Volume between -dV (a[1] = 0.0) and +dV (a[1] = 1.0)\n",
    "    #   a[2:] adds between none (a[2:] = 0.0) and all (a[2:] = 1.0) of each reactant\n",
    "    if total_steps  < 20:\n",
    "        action[0] = 1\n",
    "        action[1] = 1\n",
    "        action[2] = 0    # 1-chlorohexane\n",
    "        action[3] = 0    # 2-chlorohexane\n",
    "        action[4] = 1    # 3-chlorohexane\n",
    "        action[5] = 1    # Na\n",
    "\n",
    "        '''\n",
    "        # Adding Reactants not needed:\n",
    "        action[0] = 1\n",
    "        action[1] = 1\n",
    "        action[5] = 1\n",
    "        action[4] = 1\n",
    "        action[2] = 1\n",
    "        action[3] = 1\n",
    "        '''\n",
    "\n",
    "    # perform the action and update the reward\n",
    "    state, reward, done, __ = env.step(action)\n",
    "    print('-----------------------------------------')\n",
    "    print('total_steps: ', total_steps)\n",
    "    print('reward: %.2f ' % reward)\n",
    "    total_reward += reward\n",
    "    print('total reward: %.2f' % total_reward)\n",
    "    # print(state)\n",
    "\n",
    "    # render the plot\n",
    "    env.render(mode=render_mode)\n",
    "    # sleep(2)\n",
    "\n",
    "    # increment one step\n",
    "    total_steps += 1\n",
    "\n",
    "    # append arrays for states over time\n",
    "    reward_over_time.append(reward)\n",
    "    total_reward_over_time.append(total_reward)\n",
    "    steps_over_time.append(total_steps)\n",
    "    reactant_1.append(env.state[6])\n",
    "    reactant_2.append(env.state[7])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we get a high total reward. A visual representation of the reactants being used and total reward increasing can be seen in the subplot we produce!\n",
    "\n",
    "![subplot](../../docs/tutorial_figures/reaction-lesson-2/subplots.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simply shows us the stats of the reaction vessel. It essentially shows everything from thermodynamic variables, to the amount of material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ask user if they want to see stats of reaction vessel\n",
    "show_stats = input(\"Show Reaction Vessel Stats ('Y'/'N') >>> \")\n",
    "\n",
    "if show_stats.lower() in [\"y\", \"yes\"]:\n",
    "    # open and check the material dict\n",
    "    vessel_path = os.path.join(os.getcwd(), \"reaction_vessel.pickle\")\n",
    "    with open(vessel_path, 'rb') as open_file:\n",
    "        v = pickle.load(open_file)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"---------- VESSEL ----------\")\n",
    "    print(\"Label: {}\".format(v.label))\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"---------- THERMODYNAMIC VARIABLES ----------\")\n",
    "    print(\"Temperature (in K): {:e}\".format(v.temperature))\n",
    "    print(\"Volume (in L): {:e}\".format(v.volume))\n",
    "    print(\"Pressure (in kPa): {:e}\".format(v.pressure))\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"---------- MATERIAL_DICT ----------\")\n",
    "    for material, value_list in v._material_dict.items():\n",
    "        print(\"{} : {}\".format(material, value_list))\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"---------- SOLUTE_DICT ----------\")\n",
    "    for solute, value_list in v._solute_dict.items():\n",
    "        print(\"{} : {}\".format(solute, value_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the code plots certain states over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# graph states over time\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4)\n",
    "ax1.plot(steps_over_time, reactant_1)\n",
    "ax1.set_title('Steps vs. Reactant 1 (3-chlorohexane)')\n",
    "ax1.set_xlabel('Steps')\n",
    "ax1.set_ylabel('3-chlorohexane')\n",
    "\n",
    "ax2.plot(steps_over_time, reactant_2, 'tab:orange')\n",
    "ax2.set_title('Steps vs. Reactant 2 (Na)')\n",
    "ax2.set_xlabel('Steps')\n",
    "ax2.set_ylabel('Na')\n",
    "\n",
    "ax3.plot(steps_over_time, reward_over_time, 'tab:green')\n",
    "ax3.set_title('Steps vs Reward')\n",
    "ax3.set_xlabel('Steps')\n",
    "ax3.set_ylabel('Reward')\n",
    "\n",
    "ax4.plot(steps_over_time, total_reward_over_time, 'tab:red')\n",
    "ax4.set_title('Steps vs Total Reward')\n",
    "ax4.set_xlabel('Steps')\n",
    "ax4.set_ylabel('Total Reward')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig('Final Subplots Demo Lesson 3.png')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second part of the experiment let's uncomment the code that adds the reactants not needed and run our code again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Reactants not needed:\n",
    "action[0] = 1    # temperature\n",
    "action[1] = 1    # volume\n",
    "action[2] = 1    # 1-chlorohexane\n",
    "action[3] = 1    # 2-chlorohexane\n",
    "action[4] = 1    # 3-chlorohexane\n",
    "action[5] = 1    # Na"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If we run this code we'll notice that our reward is significantly lower. It is a significant drop-off from the reward we get from our previous set of actions. Once again, the reason this is happening is that other reactions are taking place instead of the reaction that produces our desired material.  \n",
    "\n",
    "The next step for this reaction environment is to write an RL implementation that will allow the agent to solve this problem for you essentially maximizing our output of the desired material! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ChemGym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13 |Anaconda, Inc.| (default, Feb 23 2021, 21:15:04) \n[GCC 7.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "928df2993789dc54629220469d2aa2c5bde6e75786cdddb015342ca5eb5b2bb1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
